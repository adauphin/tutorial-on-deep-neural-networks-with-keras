{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial_part1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "ecQT698A6s2P",
        "-7tzOxTH6zaK",
        "FOnxNEMr7KDV",
        "6kD6vQlV78e7"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adauphin/tutorial-on-deep-neural-networks-with-keras/blob/master/tutorial_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "rt4gwaPR6hyw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h1>Tutorial on deep neural networks and deep convolutionnal neural networks</h1>\n",
        "\n",
        "---\n",
        "\n",
        "**Alexandre Dauphin**"
      ]
    },
    {
      "metadata": {
        "id": "ecQT698A6s2P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import the libraries"
      ]
    },
    {
      "metadata": {
        "id": "9q_2DucX6dai",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "83a325a6-baa4-4199-e609-7f9243ec5571"
      },
      "cell_type": "code",
      "source": [
        "# import numpy and matplotlib\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "#import keras and tools from keras to generate neural networks and convolutionnal neural networks\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Input,Dense,Conv2D,MaxPooling2D,Flatten, Activation, BatchNormalization, GlobalAveragePooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.vgg16 import VGG16\n",
        "\n",
        "# import the dataset\n",
        "\n",
        "from keras.datasets import mnist\n",
        "\n",
        "# scikit learn\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#miscellaneous\n",
        "\n",
        "from collections import Counter"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "-7tzOxTH6zaK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Link the colab file to your google drive\n",
        "\n",
        "---\n",
        "\n",
        "By executing the next cell, we will be able to access and write data on the google drive"
      ]
    },
    {
      "metadata": {
        "id": "RJSbbxrp67nH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#link the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FOnxNEMr7KDV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MNIST dataset\n",
        "---\n",
        "We now import the MNIST hanwritten digits and explore it. We first load the data"
      ]
    },
    {
      "metadata": {
        "id": "XbU7vlEj7MbU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_classes=10 #number of classes in the MNIST dataset, numbers from 0 to 9\n",
        "\n",
        "(x_train,y_train1), (x_test,y_test1) = mnist.load_data()\n",
        "x_train1 = x_train.astype('float32') / 255.\n",
        "x_test1 = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train1, n_classes) # this keras funtion transforms the labels into vectors. For example, 2 becomes [0,0,1,0,0,0,0,0,0,0]\n",
        "y_test = np_utils.to_categorical(y_test1, n_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jCL7zfwq7g0t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We take 12 examples in the training set\n",
        "nexample=12\n",
        "mask=np.random.randint(x_train.shape[0],size=nexample)\n",
        "\n",
        "fig,axes=plt.subplots(nrows=1,ncols=nexample,figsize=(20,20))\n",
        "\n",
        "i=-1\n",
        "for ax in axes.flat:\n",
        "  i=i+1\n",
        "  nn=mask[i]\n",
        "  ax.imshow(x_train1[nn,:,:],cmap='gray_r')\n",
        "  ax.set_title('label: '+str(y_train1[nn]))\n",
        "  ax.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6kD6vQlV78e7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep neural networks (Fully connected)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "5CZ1B1Pplpne",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now construct our first neural network. The input of the neural network is a vector of 28X28=784 elements. We then make the data pass through 3 hidden fully connected neural networks with repectively 200, 64 and 10 neurons. We finally perform the classification in the last layer with a softmax function. \n",
        "\n",
        "Then, we compile model the model and we specify the Cost function (here cross entropy), the optimizer (adam, an elaborated version of the stochastic gradient descent), and the metrics.\n",
        "\n",
        "\n",
        "Your goal is now to construct the neural network architecture. We will do it in keras\n",
        "\n",
        "https://keras.io/#getting-started-30-seconds-to-keras"
      ]
    },
    {
      "metadata": {
        "id": "hpyJlkNs-gkG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gX4Ol-Aq79Et",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title Solution (double click to see the code)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(units=128, input_dim=784,activation='relu'))\n",
        "model.add(Dense(units=64,activation='relu'))\n",
        "model.add(Dense(units=10,activation='relu'))\n",
        "model.add(Dense(units=10, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oYBIxuiW9fL6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now train the model. To this end, we use the function model.fit(). The latter allows one to choose the batch size and the division between training and validation set.\n",
        "We put some checkpoints that will save the weigths of the neural network after each epoch. We also save the history after the training to have access to the learning curves (loss function and accuracy)"
      ]
    },
    {
      "metadata": {
        "id": "EzEPi3s_vnPE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This line should be put if the model has been previously trained\n",
        "#model.load_weights('drive/My Drive/weights.best.hdf5')\n",
        "# checkpoint\n",
        "filepath=\"drive/My Drive/weights_fnn.best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# Verbose: 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
        "\n",
        "history=model.fit(x_train, y_train, epochs=40, batch_size=64,validation_split=0.2,callbacks=callbacks_list,verbose=1)\n",
        "\n",
        "np.savez('drive/My Drive/history_fnn.npz',loss=history.history['loss'],val_loss=history.history['val_loss']\\\n",
        "         ,acc=history.history['acc'],val_acc=history.history['val_acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W_OMB0GZwiRr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's have a look at the learning curves. To this end, we load the history that we previously saved."
      ]
    },
    {
      "metadata": {
        "id": "0tWaM2rzwi6B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data=np.load('drive/My Drive/history_fnn.npz')\n",
        "loss,val_loss,acc,val_acc=data['loss'],data['val_loss'],data['acc'],data['val_acc']\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3,figsize=(10,5))\n",
        "\n",
        "ax=axes[0]\n",
        "ax.plot(loss,'.--',label='training')\n",
        "ax.plot(val_loss,'.--',label='validation')\n",
        "ax.set_title('Loss Function')\n",
        "ax.legend()\n",
        "\n",
        "ax=axes[1]\n",
        "ax.plot(acc,'.--',label='training')\n",
        "ax.plot(val_acc,'.--',label='validation')\n",
        "ax.set_title('Accuracy')\n",
        "ax.legend()\n",
        "\n",
        "ax=axes[2]\n",
        "vepochs=np.arange(np.size(loss))\n",
        "ax.plot(vepochs[20:],loss[20:],'.--',label='training')\n",
        "ax.plot(vepochs[20:],val_loss[20:],'.--',label='validation')\n",
        "ax.set_title('Zoom Loss')\n",
        "ax.legend();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l8wbbQ0Yw5J-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now want visualize the effect of the different layers on the original data from the test set. To this end, we first perform a PCA on the original data to visualize in a 2D plane the data.\n",
        "\n",
        "- use the PCA Function of [scikit learn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)"
      ]
    },
    {
      "metadata": {
        "id": "bi4LBtrmw5j0",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title Solution (double click to see the code)\n",
        "pca = PCA(n_components=2)\n",
        "x_original=pca.fit_transform(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XR6kmgVPgk-g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now plot the training set in the two-dimensional plane."
      ]
    },
    {
      "metadata": {
        "id": "pYLkjggLgFDW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(nrows=1, ncols=1,figsize=(8,8))\n",
        "\n",
        "im1=ax.scatter(x_original[:,0],x_original[:,1],c=y_test1,cmap='jet')\n",
        "ax.set_title('PCA on raw data')\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.colorbar(im1,ax=ax);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QDvU5cYXjJZo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now perform the same analysis after passing through one of the layer."
      ]
    },
    {
      "metadata": {
        "id": "fZ-1iZnMjLw5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This line should be put if the model has been previously trained\n",
        "#model.load_weights('drive/My Drive/weights.best.hdf5')\n",
        "\n",
        "#In get_layer(), you should insert the name of the layer you are interested in from model.summary()\n",
        "\n",
        "test_data=x_test\n",
        "test_label=y_test1\n",
        "\n",
        "layer1=Model(inputs=model.input,outputs=model.get_layer('dense_2').output)\n",
        "layer1=layer1.predict(test_data)\n",
        "\n",
        "xl1=pca.fit_transform(layer1)\n",
        "\n",
        "\n",
        "kmeans=KMeans(n_clusters=10)\n",
        "kmeans.fit(layer1)\n",
        "y_pred1=kmeans.predict(layer1)\n",
        "\n",
        "\n",
        "vchange=np.ones(10)*1000\n",
        "for i in np.arange(10):\n",
        "  vchange[i]=Counter(test_label[y_pred1==i]).most_common(1)[0][0]\n",
        "\n",
        "y_pred=np.copy(y_pred1)\n",
        "for i in np.arange(10):\n",
        "  y_pred[y_pred1==i]=vchange[i]\n",
        " \n",
        "print('accuracy score:',accuracy_score(test_label,y_pred))\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10,5))\n",
        "\n",
        "ax=axes[0]\n",
        "im1=ax.scatter(xl1[:,0],xl1[:,1],c=test_label,cmap='jet')\n",
        "ax.set_title('PCA with real labels')\n",
        "\n",
        "\n",
        "ax=axes[1]\n",
        "im2=ax.scatter(xl1[:,0],xl1[:,1],c=y_pred,cmap='jet')\n",
        "ax.set_title('PCA with k_means predicition')\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.colorbar(im2,ax=axes.ravel().tolist());"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cQi8EzM7lbsG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Convolutional neural networks\n",
        "---\n"
      ]
    },
    {
      "metadata": {
        "id": "UEWWeEt9ls1j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now focus on the classification of the MNIST dataset with convolutional neural networks. To this end, we construct a CNN with 3 convolutional layers (each of them accompanied by max Pooling layer) and two fully connected neural networks.\n",
        "\n",
        "Have a look at:\n",
        "- [CNN definition in keras](https://keras.io/layers/convolutional/)\n",
        "- [pooling layers in keras ](https://keras.io/layers/pooling/)"
      ]
    },
    {
      "metadata": {
        "id": "xbLvAAdRliVX",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title Solution (double click to see the code)\n",
        "cnn=Sequential()\n",
        "cnn.add(Conv2D(16, (3, 3), activation='relu', padding='same',input_shape=(28, 28, 1)))\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn.add(Flatten())\n",
        "cnn.add(Dense(128, activation='relu'))\n",
        "cnn.add(Dense(10, activation='relu'))\n",
        "cnn.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "cnn.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "cnn.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2eIDoDuA8Smr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notice, that due to the nature of the convolutional filters, the numbers of parameters has decreased a lot. Nevertheless, as we will see this kind of architecture gives much better results on images.\n",
        "For the training, we have to reshape the data for the keras standard (for the tensorflow backend)"
      ]
    },
    {
      "metadata": {
        "id": "2Z2j-Lfll9iE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train=x_train.reshape(60000,28,28,1)\n",
        "x_test = x_test.reshape(10000,28,28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vOmzWj928X_q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We then train the model."
      ]
    },
    {
      "metadata": {
        "id": "Aduuths_8XOJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This line should be put if the model has been previously trained\n",
        "#model.load_weights('drive/My Drive/weights.best.hdf5')\n",
        "# checkpoint\n",
        "filepath=\"drive/My Drive/weights_cnn.best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# Verbose: 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
        "\n",
        "history=cnn.fit(x_train, y_train, epochs=10, batch_size=64,validation_split=0.2,callbacks=callbacks_list,verbose=1)\n",
        "\n",
        "np.savez('drive/My Drive/history_cnn.npz',loss=history.history['loss'],val_loss=history.history['val_loss']\\\n",
        "         ,acc=history.history['acc'],val_acc=history.history['val_acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UtmCAtlv8c4S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We then plot the loss function and the accuracy. Surprisingly, after a few number of epochs, the training converges to an accuracy of 98%! This is much better than with the FNN."
      ]
    },
    {
      "metadata": {
        "id": "_Akv9XfI8dRL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data=np.load('drive/My Drive/history_cnn.npz')\n",
        "loss,val_loss,acc,val_acc=data['loss'],data['val_loss'],data['acc'],data['val_acc']\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=4,figsize=(20,5))\n",
        "\n",
        "ax=axes[0]\n",
        "ax.plot(loss,'.--',label='training')\n",
        "ax.plot(val_loss,'.--',label='validation')\n",
        "ax.set_title('Loss Function')\n",
        "ax.legend()\n",
        "\n",
        "ax=axes[1]\n",
        "ax.plot(acc,'.--',label='training')\n",
        "ax.plot(val_acc,'.--',label='validation')\n",
        "ax.set_title('Accuracy')\n",
        "ax.legend()\n",
        "\n",
        "ax=axes[2]\n",
        "vepochs=np.arange(np.size(loss))\n",
        "ax.plot(vepochs[5:],acc[5:],'.--',label='training')\n",
        "ax.plot(vepochs[5:],val_acc[5:],'.--',label='validation')\n",
        "ax.set_title('Zoom Accuracy')\n",
        "ax.legend();\n",
        "\n",
        "ax=axes[3]\n",
        "vepochs=np.arange(np.size(loss))\n",
        "ax.plot(vepochs[5:],loss[5:],'.--',label='training')\n",
        "ax.plot(vepochs[5:],val_loss[5:],'.--',label='validation')\n",
        "ax.set_title('Zoom Loss')\n",
        "ax.legend();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QBmTIsqAIxL0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can also study the layer before the classfication with PCA and k-means.\n",
        "\n",
        "Choose a convolutional layer output and perform the PCA dimension reduction."
      ]
    },
    {
      "metadata": {
        "id": "mMUTCPFqIxjH",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title Solution (double click to see the code)\n",
        "pca = PCA(n_components=2)\n",
        "layer1=Model(inputs=cnn.input,outputs=cnn.get_layer('dense_6').output)\n",
        "layer1=layer1.predict(x_test)\n",
        "\n",
        "\n",
        "# Uncomment these lines to flatten the data coming from the CNN\n",
        "#s=layer1.shape\n",
        "#n1=s[1]*s[2]*s[3]\n",
        "#layer1=layer1.reshape(s[0],n1)\n",
        "\n",
        "xl1=pca.fit_transform(layer1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L1YQyGtzJcHJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now plot the two-dimensional space and perform a k-means analysis."
      ]
    },
    {
      "metadata": {
        "id": "oMQDXu8FJZAY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "kmeans=KMeans(n_clusters=10)\n",
        "kmeans.fit(layer1)\n",
        "y_pred1=kmeans.predict(layer1)\n",
        "\n",
        "\n",
        "vchange=np.ones(10)*1000\n",
        "for i in np.arange(10):\n",
        "  vchange[i]=Counter(y_test1[y_pred1==i]).most_common(1)[0][0]\n",
        "\n",
        "y_pred=np.copy(y_pred1)\n",
        "for i in np.arange(10):\n",
        "  y_pred[y_pred1==i]=vchange[i]\n",
        " \n",
        "print('accuracy score:',accuracy_score(y_test1,y_pred))\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10,5))\n",
        "\n",
        "ax=axes[0]\n",
        "im1=ax.scatter(xl1[:,0],xl1[:,1],c=y_test1,cmap='jet')\n",
        "ax.set_title('PCA with real labels')\n",
        "\n",
        "\n",
        "ax=axes[1]\n",
        "im2=ax.scatter(xl1[:,0],xl1[:,1],c=y_pred,cmap='jet')\n",
        "ax.set_title('PCA with k_means predicition')\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.colorbar(im2,ax=axes.ravel().tolist());"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}